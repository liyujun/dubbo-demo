<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE configuration>
<configuration debug="false" scan="true" scanPeriod="15 second">
    <!--工程名若为外部读取，则需要将日志配置文件重新命名，如：logback-logstash.xml，因为logback-spring.xml是系统默认的一种命名，加载优先级高，会取不到该变量 -->
    <springProperty scope="context" name="application.name"     source="spring.application.name"     defaultValue="dubbo-consumer"/>
    <!-- 设置缓冲日志数，如果设置成0，日志发送是同步的，如果设置成大于0的值，会将日志放入队列，队列长度到达指定值，在统一发送, 可以加大服务吞吐量。 这个值只能配置2的N次幂（2,4,8,16...），默认8192，单位是B -->
    <springProperty scope="context" name="logback.queue.size"   source="logback.queue.size"          defaultValue="8192" />
    <!-- kafka attribute configuration -->
    <springProperty scope="context" name="producer.username"      source="logback.producer.username"      defaultValue="kafka" />
    <springProperty scope="context" name="producer.password"      source="logback.producer.password"      defaultValue="kafka" />
    <springProperty scope="context" name="producer.servers"       source="logback.producer.servers"       defaultValue="127.0.0.1:9092" />
    <springProperty scope="context" name="producer.topic"         source="logback.producer.topic"         defaultValue="elk-acctTestTopic" />
    <springProperty scope="context" name="producer.retries"       source="logback.producer.retries"       defaultValue="0" />
    <springProperty scope="context" name="producer.batch.size"    source="logback.producer.batch.size"    defaultValue="16000" />
    <springProperty scope="context" name="producer.linger.ms"     source="logback.producer.linger.ms"     defaultValue="0" />
    <springProperty scope="context" name="producer.buffer.memory" source="logback.producer.buffer.memory" defaultValue="33554432" />
    <springProperty scope="context" name="producer.acks"          source="logback.producer.acks"          defaultValue="0" />
    <springProperty scope="context" name="producer.max.block.ms"  source="logback.producer.max.block.ms"  defaultValue="60000" />

    <!-- file info -->
    <springProperty scope="context" name="file.size"            source="logback.file.size"           defaultValue="100MB" />
    <springProperty scope="context" name="file.max.history"     source="logback.file.max.history"    defaultValue="30" />
    <springProperty scope="context" name="file.total.size.cap"  source="logback.file.total.size.cap" defaultValue="300MB" />

    <property name="file.path"   value="logs/${application.name}/" />
    <property name="file.name"   value="${application.name}" />

    <!-- Date format -->
    <property name="date_format" value="yyyy-MM-dd HH:mm:ss.SSS" />

    <!-- This is the ConsoleAppender -->
    <appender name="consoleAppender" class="ch.qos.logback.core.ConsoleAppender">
        <encoder charset="utf-8">
            <pattern>[%-5level] %d{${date_format}} [%thread] [%X{traceId}] [%logger{36}#%M:%L] - %m%n</pattern>
        </encoder>
    </appender>


    <!-- 输入到文件，按日期和文件大小 -->
    <appender name="fileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <encoder charset="utf-8">
            <pattern>[%-5level] %d{${date_format}} [%thread] [%X{traceId}] [%logger{36}#%M:%L] - %m%n</pattern>
        </encoder>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${file.path}%d/${file.name}.%i.log</fileNamePattern>
            <maxHistory>${file.max.history}</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>${file.size}</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <totalSizeCap>${file.total.size.cap}</totalSizeCap>
        </rollingPolicy>
    </appender>

    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <!--<context />-->
                <timestamp>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                        "appname":"${application.name}",
                        "datetime":"%d{${date_format}}",
                        "level": "%-5level",
                        "thread": "%thread",
                        "class": "%logger{36}#%M:%L",
                        "message": "%m%n",
                        "stack_trace": "%exception{10}"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
        <topic>${producer.topic}</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <producerConfig>bootstrap.servers=${producer.servers}</producerConfig>
        <producerConfig>retries=${producer.retries}</producerConfig>
        <producerConfig>batch.size=${producer.batch.size}</producerConfig>
        <producerConfig>linger.ms=${producer.linger.ms}</producerConfig>
        <producerConfig>buffer.memory=${producer.buffer.memory}</producerConfig>
        <producerConfig>acks=${producer.acks}</producerConfig>
        <producerConfig>max.block.ms=${producer.max.block.ms}</producerConfig>
    </appender>

    <appender name="asyncFileAppender" class="ch.qos.logback.classic.AsyncAppender">
        <queueSize>${logback.queue.size}</queueSize>
        <neverBlock>true</neverBlock>
        <includeCallerData>true</includeCallerData>
        <appender-ref ref="fileAppender" />
    </appender>

    <appender name="asyncKafkaAppender" class="ch.qos.logback.classic.AsyncAppender">
        <queueSize>${logback.queue.size}</queueSize>
        <neverBlock>true</neverBlock>
        <includeCallerData>true</includeCallerData>
        <appender-ref ref="kafkaAppender" />
    </appender>

    <logger name="RocketmqClient" level="error"/>

    <!--
        Log level is divided from low to high: TRACE < DEBUG < INFO < WARN < ERROR < FATAL
        If set to WARN, information below WARN will not be output.
    -->
    <root level="INFO">
        <appender-ref ref="consoleAppender" />
        <appender-ref ref="asyncFileAppender" />
        <appender-ref ref="asyncKafkaAppender" />
    </root>
</configuration>
